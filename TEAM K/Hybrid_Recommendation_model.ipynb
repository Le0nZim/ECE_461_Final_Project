{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install lightfm\n",
        "!pip install surprise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQLgvB6N6bLK",
        "outputId": "2cd79bd8-ca1e-4e5c-ed4c-259bc4f6e3d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightfm in /usr/local/lib/python3.10/dist-packages (1.17)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lightfm) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from lightfm) (1.11.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from lightfm) (2.31.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from lightfm) (1.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->lightfm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->lightfm) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->lightfm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->lightfm) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->lightfm) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->lightfm) (3.2.0)\n",
            "Collecting surprise\n",
            "  Downloading surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
            "Collecting scikit-surprise (from surprise)\n",
            "  Downloading scikit-surprise-1.1.3.tar.gz (771 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.0/772.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise->surprise) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise->surprise) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise->surprise) (1.11.4)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.3-cp310-cp310-linux_x86_64.whl size=3162669 sha256=a6a18946d933d8f1286d7541086e30f39de3410f4fa473d267210efd99fdbbd0\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/ca/a8/4e28def53797fdc4363ca4af740db15a9c2f1595ebc51fb445\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise, surprise\n",
            "Successfully installed scikit-surprise-1.1.3 surprise-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HT9dHofN1y6g",
        "outputId": "88f94dd8-d45e-436b-d275-512ac43d6419"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Time: 3.921278953552246 seconds\n",
            "Hybrid Model Prediction: 3.5151494753270422\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Load MovieLens dataset\n",
        "ratings = pd.read_csv('ratings.csv')\n",
        "movies = pd.read_csv('movies.csv')\n",
        "\n",
        "# Collaborative Filtering\n",
        "reader = Reader(rating_scale=(0.5, 5.0))\n",
        "data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
        "trainset, testset = train_test_split(data, test_size=0.2)\n",
        "start_time = time.time()\n",
        "cf_model = SVD()\n",
        "cf_model.fit(trainset)\n",
        "\n",
        "# Content-Based Filtering\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(movies['genres'])\n",
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "print(f\"Training Time: {training_time} seconds\")\n",
        "\n",
        "# Hybrid Model\n",
        "def hybrid_recommendation(userId, movieId):\n",
        "    # Collaborative Filtering prediction\n",
        "    cf_prediction = cf_model.predict(userId, movieId).est\n",
        "\n",
        "    # Content-Based Filtering prediction\n",
        "    movie_idx = movies[movies['movieId'] == movieId].index[0]\n",
        "    content_based_scores = list(enumerate(cosine_sim[movie_idx]))\n",
        "    content_based_scores = sorted(content_based_scores, key=lambda x: x[1], reverse=True)\n",
        "    cb_prediction = content_based_scores[1][1]  # Considering the second most similar movie\n",
        "\n",
        "    # Weighted Hybrid Model\n",
        "    hybrid_prediction = 0.7 * cf_prediction + 0.3 * cb_prediction\n",
        "\n",
        "    return hybrid_prediction\n",
        "\n",
        "# Example usage\n",
        "userId = 1\n",
        "movieId = 47\n",
        "prediction = hybrid_recommendation(userId, movieId)\n",
        "print(f'Hybrid Model Prediction: {prediction}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preproccess"
      ],
      "metadata": {
        "id": "_suwyQOz_lHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load ratings data\n",
        "ratings = pd.read_csv('ratings.csv')\n",
        "\n",
        "# Load movies data\n",
        "movies = pd.read_csv('movies.csv')"
      ],
      "metadata": {
        "id": "hffgtAci8DHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CF with SVD"
      ],
      "metadata": {
        "id": "FKtFSgJ5Gfzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise import Reader, Dataset\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise import accuracy\n",
        "from surprise import SVD  # You can try other algorithms as well\n",
        "\n",
        "# Load data into Surprise format\n",
        "reader = Reader(rating_scale=(0.5, 5.0))\n",
        "data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
        "\n",
        "# Split data into train and test sets\n",
        "trainset, testset = train_test_split(data, test_size=0.25)\n",
        "\n",
        "# Train a collaborative filtering model\n",
        "algo = SVD()  # You can try other algorithms as well\n",
        "algo.fit(trainset)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = algo.test(testset)\n",
        "\n",
        "# Evaluate the model using MAE\n",
        "rmse = accuracy.rmse(predictions)\n",
        "print(f'Mean Absolute Error (RMSE): {rmse}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AObJ1hXx_wtt",
        "outputId": "4e1de109-d7d1-452a-837b-426d040a6f6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 0.8744\n",
            "Mean Absolute Error (RMSE): 0.8744154675222571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CB with tfidf and cosine similarity"
      ],
      "metadata": {
        "id": "L2epBZzhGjbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "# Create a TF-IDF matrix for movie genres\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(movies['genres'].fillna(''))\n",
        "\n",
        "# Calculate cosine similarity between movies\n",
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n"
      ],
      "metadata": {
        "id": "B1u160NT_3S7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hybrid Model"
      ],
      "metadata": {
        "id": "hfVYLQ-UGo2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise import accuracy\n",
        "\n",
        "# Create a function to get collaborative filtering recommendations\n",
        "def get_collab_recommendations(userId, movieId):\n",
        "    return algo.predict(userId, movieId).est\n",
        "\n",
        "# Create a function to get content-based recommendations\n",
        "def get_content_recommendations(movieId):\n",
        "    sim_scores = list(enumerate(cosine_sim[movieId]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    return sim_scores[1:11]\n",
        "\n",
        "# Create a function to make hybrid recommendations\n",
        "def hybrid_recommendations(userId, movieId):\n",
        "    collab_score = get_collab_recommendations(userId, movieId)\n",
        "    content_scores = get_content_recommendations(movieId)\n",
        "\n",
        "    # Weighted sum of collaborative and content scores\n",
        "    hybrid_scores = [(idx, collab_score + 0.2 * content_score) for idx, content_score in content_scores]\n",
        "\n",
        "    # Sort the recommendations by score\n",
        "    hybrid_scores = sorted(hybrid_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Get the top 5 movie recommendations\n",
        "    top_movies = [movies.iloc[idx]['title'] for idx, _ in hybrid_scores[:10]]\n",
        "    return top_movies\n",
        "\n",
        "# Example usage\n",
        "userId = 2\n",
        "movieId = 2\n",
        "recommendations = hybrid_recommendations(userId, movieId)\n",
        "print(recommendations)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmUq03aA_6Nc",
        "outputId": "0c1e0e02-8291-4bf7-9efe-498648e606f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sabrina (1995)', 'Clueless (1995)', 'Two if by Sea (1996)', 'French Twist (Gazon maudit) (1995)', 'If Lucy Fell (1996)', 'Boomerang (1992)', 'Pie in the Sky (1996)', 'Mallrats (1995)', 'Nine Months (1995)', 'Forget Paris (1995)']\n"
          ]
        }
      ]
    }
  ]
}