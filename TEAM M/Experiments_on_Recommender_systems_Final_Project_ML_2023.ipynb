{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuG7wepqiWYd"
      },
      "source": [
        "*cold start with input keywords  <br>\n",
        "*scale after split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHTfPnfykWhr",
        "outputId": "abe1eea0-153a-4493-e297-436d1d8b064d"
      },
      "outputs": [],
      "source": [
        "# %pip install scikit-surprise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F-yiPOTVZ0Zu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from surprise import SVD, SVDpp\n",
        "from surprise import SlopeOne, BaselineOnly\n",
        "from surprise import accuracy\n",
        "from surprise import Reader\n",
        "from surprise import Dataset\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from surprise import KNNBaseline, KNNWithZScore,KNNWithMeans\n",
        "from surprise.model_selection import cross_validate\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vdRZXaQootXA"
      },
      "outputs": [],
      "source": [
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gMpUrgg2a-7c"
      },
      "outputs": [],
      "source": [
        "ratings_df = pd.read_csv('C:/Users/User/Documents/#M.L/2023/Final Project ML 2023 Recommender Systems/Datasets/ml-latest-small/ratings.csv')\n",
        "movies_df  = pd.read_csv('C:/Users/User/Documents/#M.L/2023/Final Project ML 2023 Recommender Systems/Datasets/ml-latest-small/movies.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9J-3mXVlRWeV"
      },
      "outputs": [],
      "source": [
        "# ratings_df = pd.read_table('C:/Users/User/Documents/#M.L/2023/Final Project ML 2023 Recommender Systems/Datasets/ml-1m/ratings.dat', delimiter=\"::\", engine='python', header=None, names=['userId','movieId','rating','timestamp'])\n",
        "# movies_df  = pd.read_table('C:/Users/User/Documents/#M.L/2023/Final Project ML 2023 Recommender Systems/Datasets/ml-1m/movies.dat' , encoding='latin-1', delimiter=\"::\", engine='python', header=None, names=['movieId','title','genres'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zeY73IqcX0Hu"
      },
      "outputs": [],
      "source": [
        "# ratings_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ml-10M/ratings.dat',delimiter=\"::\",engine='python', header=None, names=['userId', 'movieId', 'rating', 'timestamp'])\n",
        "# movies_df  = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ml-10M/movies.dat' ,delimiter=\"::\",engine='python', header=None, names=['movieId', 'title', 'genres'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "E9rwum-Y4sUs"
      },
      "outputs": [],
      "source": [
        "# ratings_df = pd.read_csv('C:/Users/User/Documents/#M.L/2023/Final Project ML 2023 Recommender Systems/Datasets/ml-latest/ratings.csv')\n",
        "# movies_df  = pd.read_csv('C:/Users/User/Documents/#M.L/2023/Final Project ML 2023 Recommender Systems/Datasets/ml-latest/movies.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "t6S_dWAAX7z8"
      },
      "outputs": [],
      "source": [
        "complete_dataset = pd.merge(ratings_df,movies_df,on='movieId')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWzXtuSen4Q4"
      },
      "source": [
        "### Converting pandas DataFrame to Surprise DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "H2ipSxc7n3Km"
      },
      "outputs": [],
      "source": [
        "# split the dataset\n",
        "# trainset_df, testset_df = complete_dataset.loc[complete_dataset.userId != \"1\"],complete_dataset.loc[complete_dataset.userId == \"1\"]\n",
        "# trainset_df, testset_df = train_test_split(complete_dataset, test_size=0.1)\n",
        "\n",
        "# create a dummy reader\n",
        "reader = Reader()\n",
        "\n",
        "# transform the two datasets\n",
        "data_sdf = Dataset.load_from_df(complete_dataset[['userId', 'movieId', 'rating']], reader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mjw5UlxvCm-Q"
      },
      "outputs": [],
      "source": [
        "data = data_sdf.build_full_trainset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0YCnGYlLs58"
      },
      "source": [
        "### Calculating the nesesary bias lists for the biased SVD\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTLEivqlXEz8"
      },
      "source": [
        "### K-NN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "p-RVD1w07_V2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*******************    k-NN Baseline   *******************\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Evaluating RMSE, MAE of algorithm KNNBaseline on 10 split(s).\n",
            "\n",
            "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Fold 6  Fold 7  Fold 8  Fold 9  Fold 10 Mean    Std     \n",
            "RMSE (testset)    0.8806  0.8683  0.8683  0.8670  0.8738  0.8631  0.8765  0.8761  0.8604  0.8686  0.8703  0.0060  \n",
            "MAE (testset)     0.6743  0.6617  0.6631  0.6597  0.6685  0.6586  0.6670  0.6709  0.6588  0.6607  0.6643  0.0052  \n",
            "Fit time          1.49    1.39    1.39    1.39    1.96    1.38    1.39    1.38    1.39    1.37    1.45    0.17    \n",
            "Test time         1.22    1.23    1.36    1.27    1.38    1.23    1.24    1.21    1.23    1.23    1.26    0.06    \n",
            " k-NN Baseline Cross Validation time 29.797332525253296\n",
            "*******************    k-NN Z-Score   *******************\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Evaluating RMSE, MAE of algorithm KNNWithZScore on 10 split(s).\n",
            "\n",
            "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Fold 6  Fold 7  Fold 8  Fold 9  Fold 10 Mean    Std     \n",
            "RMSE (testset)    0.8926  0.8962  0.8912  0.8756  0.8850  0.8882  0.8672  0.8819  0.8880  0.8995  0.8865  0.0092  \n",
            "MAE (testset)     0.6738  0.6722  0.6731  0.6657  0.6623  0.6724  0.6579  0.6688  0.6727  0.6797  0.6698  0.0060  \n",
            "Fit time          1.17    1.19    1.26    1.18    1.19    1.28    1.19    1.20    1.20    1.24    1.21    0.03    \n",
            "Test time         1.30    1.17    1.30    1.35    1.16    1.49    1.22    1.24    1.17    1.19    1.26    0.10    \n",
            " k-NN Z-Score Cross Validation time 27.302751541137695\n",
            "*******************    k-NN Means   *******************\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Evaluating RMSE, MAE of algorithm KNNWithMeans on 10 split(s).\n",
            "\n",
            "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Fold 6  Fold 7  Fold 8  Fold 9  Fold 10 Mean    Std     \n",
            "RMSE (testset)    0.8900  0.8874  0.8963  0.8791  0.8776  0.8922  0.8862  0.9049  0.8943  0.8850  0.8893  0.0077  \n",
            "MAE (testset)     0.6790  0.6756  0.6780  0.6699  0.6680  0.6761  0.6760  0.6919  0.6827  0.6758  0.6773  0.0063  \n",
            "Fit time          1.16    1.07    1.07    1.08    1.05    1.07    1.24    1.06    1.06    1.15    1.10    0.06    \n",
            "Test time         1.10    1.23    1.10    1.09    1.07    1.09    1.36    1.27    1.11    1.14    1.16    0.09    \n",
            " k-NN Means Cross Validation time 24.885639905929565\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from surprise.model_selection import cross_validate\n",
        "from surprise.prediction_algorithms.knns import KNNBaseline, KNNWithZScore, KNNWithMeans\n",
        "\n",
        "# Define k-NN models\n",
        "knn_models = [\n",
        "    KNNBaseline(k=50, sim_options={'name': 'pearson', 'user_based': True}, user_based=False),\n",
        "    KNNWithZScore(k=50, sim_options={'name': 'pearson', 'user_based': True}, user_based=False),\n",
        "    KNNWithMeans(k=50, sim_options={'name': 'pearson', 'user_based': True}, user_based=False),\n",
        "]\n",
        "\n",
        "knn_model_names = [\"k-NN Baseline\", \"k-NN Z-Score\", \"k-NN Means\"]\n",
        "\n",
        "# Loop through k-NN models\n",
        "for model, model_name in zip(knn_models, knn_model_names):\n",
        "    print(f\"*******************    {model_name}   *******************\")\n",
        "    start = time.time()\n",
        "    cross_validate(model, data_sdf, measures=['RMSE', 'MAE'], cv=10, verbose=True)\n",
        "    end = time.time()\n",
        "    model_time = end - start\n",
        "    print(f\" {model_name} Cross Validation time {model_time}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpolv_8jIKO0"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tLquvcE9IK7T"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Baseline SVD Training time 8.779996156692505 sec\n"
          ]
        }
      ],
      "source": [
        "sp1 = SlopeOne()\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "sp1.fit(data)\n",
        "end = time.time()\n",
        "print(f' Baseline SVD Training time {end-start} sec')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCTkoXq2J9H3"
      },
      "source": [
        "### Baseline Only\n",
        "its a basic prediction algorythm, we use it as a base to calculate the improvements of the rest algorithms upon."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qH4xRzAXJ82h"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Baseline SVD Training time 0.36199450492858887 sec\n"
          ]
        }
      ],
      "source": [
        "baseline = BaselineOnly(verbose=False)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "baseline.fit(data)\n",
        "end = time.time()\n",
        "print(f' Baseline SVD Training time {end-start} sec')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DU2SKUrICPV"
      },
      "source": [
        "### Single Variable Decomposition\n",
        "3 viariatios upon the popular matrix factorisation algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PW5UuKbkmcBM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Baseline SVD Training time 1.9139907360076904 sec\n"
          ]
        }
      ],
      "source": [
        "svd = SVD(verbose=False, n_epochs=20, random_state=seed)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "svd.fit(data)\n",
        "end = time.time()\n",
        "print(f' Baseline SVD Training time {end-start} sec')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gRCJfbSQ8JE7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Baseline SVD Training time 330.7479965686798 sec\n"
          ]
        }
      ],
      "source": [
        "svdpp = SVDpp(verbose=False, n_epochs=20, random_state=seed, cache_ratings=False)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "svdpp.fit(data)\n",
        "end = time.time()\n",
        "print(f' Baseline SVD Training time {end-start} sec')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_K7pm9xKKJ4z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Baseline SVD Training time 245.40833234786987 sec\n"
          ]
        }
      ],
      "source": [
        "svdppc = SVDpp(verbose=False, n_epochs=20, random_state=seed, cache_ratings=True)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "svdppc.fit(data)\n",
        "end = time.time()\n",
        "print(f' Baseline SVD Training time {end-start} sec')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_zzvSbfEIina"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " *******************   SlopeOne    *******************\n",
            "Evaluating RMSE, MAE of algorithm SlopeOne on 10 split(s).\n",
            "\n",
            "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Fold 6  Fold 7  Fold 8  Fold 9  Fold 10 Mean    Std     \n",
            "RMSE (testset)    0.8975  0.8942  0.8942  0.9012  0.8936  0.8844  0.8950  0.8826  0.9050  0.9011  0.8949  0.0067  \n",
            "MAE (testset)     0.6866  0.6861  0.6824  0.6848  0.6857  0.6751  0.6838  0.6778  0.6865  0.6842  0.6833  0.0037  \n",
            "Fit time          8.15    7.17    7.44    7.16    7.39    7.79    7.89    7.34    7.43    7.31    7.51    0.31    \n",
            "Test time         4.72    4.36    4.69    4.52    4.40    4.31    4.44    4.38    4.56    4.45    4.48    0.13    \n",
            " SlopeOne Cross Validation time 122.3729977607727\n",
            " *******************   BaselineOnly    *******************\n",
            "Evaluating RMSE, MAE of algorithm BaselineOnly on 10 split(s).\n",
            "\n",
            "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Fold 6  Fold 7  Fold 8  Fold 9  Fold 10 Mean    Std     \n",
            "RMSE (testset)    0.8600  0.8679  0.8728  0.8725  0.8654  0.8800  0.8709  0.8649  0.8703  0.8761  0.8701  0.0055  \n",
            "MAE (testset)     0.6643  0.6701  0.6735  0.6702  0.6695  0.6755  0.6679  0.6699  0.6710  0.6727  0.6705  0.0029  \n",
            "Fit time          0.44    0.43    0.45    0.43    0.39    0.39    0.40    0.40    0.40    0.39    0.41    0.02    \n",
            "Test time         0.08    0.10    0.08    0.06    0.06    0.06    0.06    0.06    0.06    0.06    0.07    0.01    \n",
            " BaselineOnly Cross Validation time 7.682999849319458\n",
            " *******************   Base SVD    *******************\n",
            "Evaluating RMSE, MAE of algorithm SVD on 10 split(s).\n",
            "\n",
            "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Fold 6  Fold 7  Fold 8  Fold 9  Fold 10 Mean    Std     \n",
            "RMSE (testset)    0.8715  0.8692  0.8664  0.8696  0.8794  0.8726  0.8659  0.8687  0.8587  0.8756  0.8698  0.0054  \n",
            "MAE (testset)     0.6709  0.6712  0.6667  0.6661  0.6736  0.6677  0.6642  0.6662  0.6591  0.6719  0.6678  0.0041  \n",
            "Fit time          1.76    1.73    1.77    1.75    1.78    1.78    1.79    1.84    1.86    1.77    1.78    0.04    \n",
            "Test time         0.12    0.11    0.12    0.11    0.11    0.30    0.12    0.15    0.12    0.11    0.14    0.06    \n",
            " Base SVD Cross Validation time 21.732004404067993\n",
            " *******************   SVDpp    *******************\n",
            "Evaluating RMSE, MAE of algorithm SVDpp on 10 split(s).\n",
            "\n",
            "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Fold 6  Fold 7  Fold 8  Fold 9  Fold 10 Mean    Std     \n",
            "RMSE (testset)    0.8514  0.8593  0.8737  0.8543  0.8506  0.8512  0.8505  0.8610  0.8573  0.8495  0.8559  0.0071  \n",
            "MAE (testset)     0.6527  0.6575  0.6689  0.6534  0.6481  0.6515  0.6536  0.6570  0.6572  0.6534  0.6553  0.0053  \n",
            "Fit time          277.83  270.22  281.28  276.77  275.84  274.52  277.21  297.10  278.45  274.54  278.38  6.83    \n",
            "Test time         9.46    9.56    9.75    9.12    9.05    9.98    9.66    9.22    9.64    9.21    9.47    0.29    \n",
            " SVDpp Cross Validation time 2880.9445927143097\n",
            " *******************   Cached SVDpp    *******************\n",
            "Evaluating RMSE, MAE of algorithm SVDpp on 10 split(s).\n",
            "\n",
            "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Fold 6  Fold 7  Fold 8  Fold 9  Fold 10 Mean    Std     \n",
            "RMSE (testset)    0.8647  0.8433  0.8558  0.8545  0.8563  0.8504  0.8626  0.8494  0.8579  0.8424  0.8537  0.0070  \n",
            "MAE (testset)     0.6600  0.6503  0.6564  0.6523  0.6553  0.6502  0.6612  0.6476  0.6585  0.6493  0.6541  0.0046  \n",
            "Fit time          199.44  201.20  201.89  201.33  206.74  224.43  209.28  202.69  221.13  198.80  206.69  8.62    \n",
            "Test time         9.31    12.87   9.85    9.32    10.34   11.08   9.94    9.38    9.39    9.15    10.06   1.09    \n",
            " Cached SVDpp Cross Validation time 2169.926887989044\n"
          ]
        }
      ],
      "source": [
        "models = [sp1, baseline, svd, svdpp, svdppc]\n",
        "model_names = [\"SlopeOne\", \"BaselineOnly\", \"Base SVD\", \"SVDpp\", \"Cached SVDpp\"]\n",
        "\n",
        "for model, model_name in zip(models, model_names):\n",
        "    print(f\" *******************   {model_name}    *******************\")\n",
        "    start = time.time()\n",
        "    cross_validate(model, data_sdf, measures=['RMSE', 'MAE'], cv=10, verbose=True)\n",
        "    end = time.time()\n",
        "    print(f\" {model_name} Cross Validation time {end - start}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install -q tensorflow-model-optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.0 0.5\n"
          ]
        }
      ],
      "source": [
        "print(ratings_df.rating.max(), ratings_df.rating.min())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "\"['timestamp'] not found in axis\"",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ratings_df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:5344\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5197\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5198\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5205\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5206\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5208\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5209\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5342\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5343\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[0;32m   5345\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   5346\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   5347\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   5348\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   5349\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   5350\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[0;32m   5351\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   5352\u001b[0m     )\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\generic.py:4711\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4709\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4710\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4711\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4714\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\generic.py:4753\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4751\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4753\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4754\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4756\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4757\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:7000\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   6999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7000\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7001\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7002\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
            "\u001b[1;31mKeyError\u001b[0m: \"['timestamp'] not found in axis\""
          ]
        }
      ],
      "source": [
        "ratings_df.drop(columns=[\"timestamp\"], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jXmNllR3z-yu"
      },
      "outputs": [],
      "source": [
        "matrix_df = ratings_df.pivot(index='userId', columns='movieId', values='rating')\n",
        "\n",
        "matrix_df = matrix_df.reset_index()\n",
        "matrix_df.drop(columns=[\"userId\"], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "610 9724 9742 (610, 9724)\n"
          ]
        }
      ],
      "source": [
        "print(len(ratings_df.userId.unique()),\n",
        "      len(ratings_df.movieId.unique()),\n",
        "      len(movies_df.movieId.unique()),\n",
        "      matrix_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "ratings_to_scale = matrix_df.iloc[:, 0:]\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_ratings = scaler.fit_transform(ratings_to_scale)\n",
        "\n",
        "matrix_df.iloc[:, 0:] = scaled_ratings\n",
        "\n",
        "train_df, test_df = train_test_split(matrix_df, test_size=0.2, random_state=42)\n",
        "n_users = len(ratings_df.userId.unique())\n",
        "n_movies = len(ratings_df.movieId.unique())\n",
        "\n",
        "full_index_values  = matrix_df.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "nan_replace = 0.5\n",
        "\n",
        "train_mask = ~np.isnan(train_df)\n",
        "test_mask = ~np.isnan(test_df)\n",
        "full_mask = ~np.isnan(matrix_df)\n",
        "\n",
        "train = train_df.fillna(nan_replace)\n",
        "test = test_df.fillna(nan_replace)\n",
        "full = matrix_df.fillna(nan_replace)\n",
        "\n",
        "train_np = train.to_numpy()\n",
        "test_np  = test.to_numpy()\n",
        "full_np = full.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: tensorflow in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (2.15.0)\n",
            "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.26.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.23.4)\n",
            "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (69.0.2)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.7.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.26.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Custom Loss Function to calculate loss only for the existing data and not the data that we filled above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from keras.losses import mean_squared_error\n",
        "from keras import backend\n",
        "\n",
        "def masked_mse(y_true, y_pred):\n",
        "    mask = backend.cast(backend.not_equal(y_true, 0), backend.floatx())\n",
        "    return mean_squared_error(y_true * mask, y_pred * mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "import numpy as np\n",
        "\n",
        "# Create the autoencoder\n",
        "# creat the encoder\n",
        "encoder_in = Input(shape=(full_np.shape[1],))\n",
        "encoder1 = Dense(255, activation='relu')(encoder_in)\n",
        "# encoder2 = Dense(128, activation='relu')(encoder1)\n",
        "encoder_out = Dense(64, activation='linear')(encoder1)\n",
        "encoderM = Model(inputs=encoder_in, outputs=encoder_out)\n",
        "encoderM.compile(optimizer='adam', loss=masked_mse)\n",
        "\n",
        "\n",
        "# creat the decoder\n",
        "decoder_in = Input(shape=(64,))\n",
        "decoder1 = Dense(64, activation='relu')(decoder_in)\n",
        "# decoder2 = Dense(128, activation='relu')(decoder1)\n",
        "decoder_out = Dense(n_movies, activation='linear')(decoder1)\n",
        "decoderM = Model(inputs=decoder_in, outputs=decoder_out)\n",
        "decoderM.compile(optimizer='adam', loss=masked_mse)\n",
        "\n",
        "\n",
        "autoencoder = Model(inputs=encoder_in, outputs=decoderM(encoderM(encoder_in)))\n",
        "autoencoder.compile(optimizer='adam', loss=masked_mse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compiling with the costom loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n",
            "20/20 [==============================] - 4s 64ms/step - loss: 0.0701\n",
            "Epoch 2/10\n",
            "20/20 [==============================] - 1s 59ms/step - loss: 0.0032\n",
            "Epoch 3/10\n",
            "20/20 [==============================] - 1s 66ms/step - loss: 0.0017\n",
            "Epoch 4/10\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 0.0013\n",
            "Epoch 5/10\n",
            "20/20 [==============================] - 1s 59ms/step - loss: 0.0013\n",
            "Epoch 6/10\n",
            "20/20 [==============================] - 1s 62ms/step - loss: 0.0013\n",
            "Epoch 7/10\n",
            "20/20 [==============================] - 1s 63ms/step - loss: 0.0013\n",
            "Epoch 8/10\n",
            "20/20 [==============================] - 1s 61ms/step - loss: 0.0013\n",
            "Epoch 9/10\n",
            "20/20 [==============================] - 1s 60ms/step - loss: 0.0014\n",
            "Epoch 10/10\n",
            "20/20 [==============================] - 1s 70ms/step - loss: 0.0013\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x17a52b34990>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "autoencoder.fit(full_np, full_np, epochs=10, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tesing prooning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from tensorflow_model_optimization.python.core.sparsity.keras import pruning_schedule\n",
        "# from tensorflow_model_optimization.python.core.sparsity.keras import pruning\n",
        "# from tensorflow_model_optimization import update_pruning_state\n",
        "\n",
        "# # Assuming you have already defined and compiled your autoencoder, encoder, and decoder\n",
        "\n",
        "# # Create a pruning schedule (you can customize the parameters)\n",
        "# pruning_schedule = pruning_schedule.ConstantSparsity(0.5, begin_step=0, frequency=100)\n",
        "\n",
        "# # Define a pruning callback for the encoder\n",
        "# pruning_callback = pruning.UpdatePruningStep()\n",
        "\n",
        "# # Prune the encoder\n",
        "# pruned_encoder = prune.prune_low_magnitude(encoderM, pruning_schedule=pruning_schedule)\n",
        "\n",
        "# # Re-compile the pruned encoder\n",
        "# pruned_encoder.compile(optimizer='adam', loss=mean_squared_error)\n",
        "\n",
        "# # Train the pruned encoder\n",
        "# pruned_encoder.fit(x_train, x_train, epochs=10, batch_size=32, validation_data=(x_val, x_val))\n",
        "\n",
        "# # Strip pruning wrappers to get the final pruned encoder\n",
        "# pruned_encoder = prune.strip_pruning(pruned_encoder)\n",
        "\n",
        "# # Now you can use the pruned encoder in your autoencoder\n",
        "# autoencoder_in = Input(shape=(full_np.shape[1],))\n",
        "# encoded_representation = pruned_encoder(autoencoder_in)\n",
        "# decoded_representation = decoderM(encoded_representation)\n",
        "# pruned_autoencoder = Model(inputs=autoencoder_in, outputs=decoded_representation)\n",
        "\n",
        "# # Compile the pruned autoencoder\n",
        "# pruned_autoencoder.compile(optimizer='adam', loss=mean_squared_error)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trying custom k-Folds method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function validate_parameter_constraints at 0x000002A72619B060> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function validate_parameter_constraints at 0x000002A72619B060> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "ename": "NotImplementedError",
          "evalue": "in user code:\n\n    File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_8620\\1917560111.py\", line 7, in masked_mse  *\n        return mean_squared_error(y_true * mask, y_pred * mask)\n    File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 435, in wrapper  *\n        return func(*args, **kwargs)\n    File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_regression.py\", line 473, in mean_squared_error  *\n        y_type, y_true, y_pred, multioutput = _check_reg_targets(\n    File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_regression.py\", line 99, in _check_reg_targets  *\n        check_consistent_length(y_true, y_pred)\n    File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py\", line 405, in check_consistent_length  *\n        uniques = np.unique(lengths)\n    File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\lib\\arraysetops.py\", line 272, in unique  **\n        ar = np.asanyarray(ar)\n\n    NotImplementedError: Cannot convert a symbolic tf.Tensor (masked_mse/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Create and train the model\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m autoencoder\u001b[38;5;241m.\u001b[39mfit(X_train, X_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Record end time\u001b[39;00m\n\u001b[0;32m     43\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filewjth7941.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filekm2vg6j_.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__masked_mse\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(mean_squared_error), (ag__\u001b[38;5;241m.\u001b[39mld(y_true) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(mask), ag__\u001b[38;5;241m.\u001b[39mld(y_pred) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(mask)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filecoo08d05.py:56\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     54\u001b[0m params \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     55\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunc_sig\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mld(global_skip_validation), if_body, else_body, get_state, set_state, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdo_return\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretval_\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fscope\u001b[38;5;241m.\u001b[39mret(retval_, do_return)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filecoo08d05.py:45\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__wrapper.<locals>.else_body\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(func), \u001b[38;5;28mtuple\u001b[39m(ag__\u001b[38;5;241m.\u001b[39mld(args)), \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(kwargs)), fscope)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileqr6o7zce.py:66\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__mean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m     64\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     65\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 66\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(_check_reg_targets), (ag__\u001b[38;5;241m.\u001b[39mld(y_true), ag__\u001b[38;5;241m.\u001b[39mld(y_pred), ag__\u001b[38;5;241m.\u001b[39mld(multioutput)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     67\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(check_consistent_length), (ag__\u001b[38;5;241m.\u001b[39mld(y_true), ag__\u001b[38;5;241m.\u001b[39mld(y_pred), ag__\u001b[38;5;241m.\u001b[39mld(sample_weight)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     68\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(np)\u001b[38;5;241m.\u001b[39maverage, ((ag__\u001b[38;5;241m.\u001b[39mld(y_true) \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(y_pred)) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m,), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(sample_weight)), fscope)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filep0wx9vhu.py:43\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m     41\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     42\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 43\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(check_consistent_length), (ag__\u001b[38;5;241m.\u001b[39mld(y_true), ag__\u001b[38;5;241m.\u001b[39mld(y_pred)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     44\u001b[0m y_true \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(check_array), (ag__\u001b[38;5;241m.\u001b[39mld(y_true),), \u001b[38;5;28mdict\u001b[39m(ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(dtype)), fscope)\n\u001b[0;32m     45\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(check_array), (ag__\u001b[38;5;241m.\u001b[39mld(y_pred),), \u001b[38;5;28mdict\u001b[39m(ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(dtype)), fscope)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file39c62ytl.py:18\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__check_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mFunctionScope(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheck_consistent_length\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfscope\u001b[39m\u001b[38;5;124m'\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mSTD) \u001b[38;5;28;01mas\u001b[39;00m fscope:\n\u001b[0;32m     17\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m [ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(_num_samples), (ag__\u001b[38;5;241m.\u001b[39mld(X),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(arrays) \u001b[38;5;28;01mif\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(X) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m---> 18\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(np)\u001b[38;5;241m.\u001b[39munique, (ag__\u001b[38;5;241m.\u001b[39mld(lengths),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state\u001b[39m():\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ()\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\lib\\arraysetops.py:272\u001b[0m, in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_unique_dispatcher)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique\u001b[39m(ar, return_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, return_inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    140\u001b[0m            return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, equal_nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03m    Find the unique elements of an array.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    270\u001b[0m \n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 272\u001b[0m     ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(ar)\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m         ret \u001b[38;5;241m=\u001b[39m _unique1d(ar, return_index, return_inverse, return_counts, \n\u001b[0;32m    275\u001b[0m                         equal_nan\u001b[38;5;241m=\u001b[39mequal_nan)\n",
            "\u001b[1;31mNotImplementedError\u001b[0m: in user code:\n\n    File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_8620\\1917560111.py\", line 7, in masked_mse  *\n        return mean_squared_error(y_true * mask, y_pred * mask)\n    File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 435, in wrapper  *\n        return func(*args, **kwargs)\n    File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_regression.py\", line 473, in mean_squared_error  *\n        y_type, y_true, y_pred, multioutput = _check_reg_targets(\n    File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_regression.py\", line 99, in _check_reg_targets  *\n        check_consistent_length(y_true, y_pred)\n    File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py\", line 405, in check_consistent_length  *\n        uniques = np.unique(lengths)\n    File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\lib\\arraysetops.py\", line 272, in unique  **\n        ar = np.asanyarray(ar)\n\n    NotImplementedError: Cannot convert a symbolic tf.Tensor (masked_mse/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend\n",
        "import time\n",
        "\n",
        "# Define masked MSE and RMSE functions\n",
        "def masked_mse(y_true, y_pred):\n",
        "    mask = backend.cast(backend.not_equal(y_true, 0), backend.floatx())\n",
        "    return mean_squared_error(y_true * mask, y_pred * mask)\n",
        "\n",
        "def masked_rmse(y_true, y_pred):\n",
        "    mse = masked_mse(y_true, y_pred)\n",
        "    return np.sqrt(mse)\n",
        "\n",
        "def masked_mae(y_true, y_pred):\n",
        "    mask = backend.cast(backend.not_equal(y_true, 0), backend.floatx())\n",
        "    return mean_absolute_error(y_true * mask, y_pred * mask)\n",
        "\n",
        "num_folds = 10\n",
        "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
        "kf.get_n_splits(full_np)\n",
        "\n",
        "# Placeholder for results\n",
        "mae_scores = []\n",
        "rmse_scores = []\n",
        "training_times = []\n",
        "\n",
        "# Perform K-Fold cross-validation\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(full_np), 1):\n",
        "    X_train, X_test = full_np[train_index], full_np[test_index]\n",
        "\n",
        "    # Record start time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = autoencoder.predict(X_test)\n",
        "\n",
        "    # Record end time\n",
        "    end_time = time.time()\n",
        "\n",
        "\n",
        "    # Calculate MSE and RMSE for this fold with custom masking\n",
        "    mse_fold = masked_mae(X_test, y_pred)\n",
        "    rmse_fold = masked_rmse(X_test, y_pred)\n",
        "\n",
        "    mae_scores.append(mse_fold)\n",
        "    rmse_scores.append(rmse_fold)\n",
        "    training_times.append(end_time - start_time)\n",
        "\n",
        "    # Print MSE, RMSE, and training time for this fold\n",
        "    print(f\"Fold {fold}: MAE = {mse_fold:.4f}, RMSE = {rmse_fold:.4f}, Testing Time = {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Calculate and print the mean and standard deviation for MSE, RMSE, and training times\n",
        "mean_mse = np.mean(mae_scores)\n",
        "std_mse = np.std(mae_scores)\n",
        "mean_rmse = np.mean(rmse_scores)\n",
        "std_rmse = np.std(rmse_scores)\n",
        "mean_training_time = np.mean(training_times)\n",
        "std_training_time = np.std(training_times)\n",
        "\n",
        "print(f\"\\nMean MAE across {num_folds} folds: {mean_mse:.4f}  {std_mse:.4f}\")\n",
        "print(f\"Mean RMSE across {num_folds} folds: {mean_rmse:.4f}  {std_rmse:.4f}\")\n",
        "print(f\"Mean Testing Time across {num_folds} folds: {mean_training_time:.2f}  {std_training_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20/20 [==============================] - 0s 14ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions = autoencoder.predict(full_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "newMovie_predictions = predictions.copy()\n",
        "newMovie_predictions[full_mask] = np.nan # Make already wached movies prediction Nan \n",
        "column_names = movies_df['title'].values.tolist()[:(newMovie_predictions.shape[1])]\n",
        "newMovie_predictions_df = pd.DataFrame(data=newMovie_predictions, columns=column_names)\n",
        "newMovie_predictions_df.set_index(pd.Index(full_index_values), inplace=True)\n",
        "newMovie_predictions_T =newMovie_predictions_df.T\n",
        "# newMovie_predictions_T.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You entered: 100\n",
            "UserID is in the list.\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    try:\n",
        "        userID = int(input(\"Enter userID: \"))\n",
        "        print(\"You entered:\", userID)\n",
        "\n",
        "        # Check if userID is in the list\n",
        "        if userID in full_index_values:\n",
        "            print(\"UserID is in the list.\")\n",
        "            break  # Break the loop if a valid ID is entered\n",
        "        else:\n",
        "            print(\"UserID is not in the list. Please try again.\")\n",
        "\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Please enter a valid integer.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>100</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Shawshank Redemption, The (1994)</th>\n",
              "      <td>3.249163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Forrest Gump (1994)</th>\n",
              "      <td>3.149484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Silence of the Lambs, The (1991)</th>\n",
              "      <td>3.128417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pulp Fiction (1994)</th>\n",
              "      <td>3.111966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Walk on the Moon, A (1999)</th>\n",
              "      <td>2.994485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Star Wars: Episode IV - A New Hope (1977)</th>\n",
              "      <td>2.991528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Jurassic Park (1993)</th>\n",
              "      <td>2.987076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Braveheart (1995)</th>\n",
              "      <td>2.956501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Toy Story (1995)</th>\n",
              "      <td>2.945926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Princess Bride, The (1987)</th>\n",
              "      <td>2.927420</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                100\n",
              "Shawshank Redemption, The (1994)           3.249163\n",
              "Forrest Gump (1994)                        3.149484\n",
              "Silence of the Lambs, The (1991)           3.128417\n",
              "Pulp Fiction (1994)                        3.111966\n",
              "Walk on the Moon, A (1999)                 2.994485\n",
              "Star Wars: Episode IV - A New Hope (1977)  2.991528\n",
              "Jurassic Park (1993)                       2.987076\n",
              "Braveheart (1995)                          2.956501\n",
              "Toy Story (1995)                           2.945926\n",
              "Princess Bride, The (1987)                 2.927420"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "user_rest = newMovie_predictions_df.index.difference([userID])\n",
        "top_n = 10  # Number of top recommendations\n",
        "\n",
        "newMovie_predictions_user = newMovie_predictions_T.drop(user_rest, axis=1, inplace=False)\n",
        "newMovie_predictions_user = newMovie_predictions_user.dropna()\n",
        "\n",
        "newMovie_predictions_user.sort_values(by=userID, ascending=False)[:top_n]*5"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
